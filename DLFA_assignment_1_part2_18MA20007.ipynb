{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DLFA_assignment_1_part2_18MA20007.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN7jK6EvSqpGjuesWwcLgSc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aparna-Sakshi/DLFA/blob/main/DLFA_assignment_1_part2_18MA20007.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1LCyHlSn0_L"
      },
      "source": [
        "# Assignment-1 : Training a neural network using back propagation\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### Name: Aparna Sakshi\n",
        "### Roll No.:18MA20007"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpZiR842ea0Y"
      },
      "source": [
        "#### Importing all the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsqtFFN3pA4P"
      },
      "source": [
        "%matplotlib inline\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torchvision import transforms,datasets\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import torchvision\n",
        "\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torchvision"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92lNuRktekNy"
      },
      "source": [
        "#### Downloading MNIST Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I32fVHqDYjAd"
      },
      "source": [
        "apply_transform = transforms.Compose([transforms.Resize(32), transforms.ToTensor()])\n",
        "BatchSizes = [256,512,1024] # change according to system specs\n",
        "colours=['blue', 'red', 'green']\n",
        "no_of_batch_sizes=len(BatchSizes)\n",
        "\n",
        "\n",
        "trainsets=[]\n",
        "trainLoaders=[]\n",
        "testsets=[]\n",
        "testLoaders=[]\n",
        "\n",
        "for BatchSize in BatchSizes:  \n",
        "  trainset = datasets.MNIST(root='./MNIST', train=True, download=True, transform=apply_transform)\n",
        "  trainLoader = torch.utils.data.DataLoader(trainset, batch_size=BatchSize,\n",
        "                                            shuffle=True, num_workers=4) # Creating dataloader\n",
        "\n",
        "  # Validation set with random rotations in the range [-90,90]\n",
        "  testset = datasets.MNIST(root='./MNIST', train=False, download=True, transform=apply_transform)\n",
        "  testLoader = torch.utils.data.DataLoader(testset, batch_size=BatchSize,\n",
        "                                          shuffle=False, num_workers=4) # Creating dataloader\n",
        "\n",
        "  trainsets.append(trainset)\n",
        "  trainLoaders.append(trainLoader)\n",
        "  testsets.append(testset)\n",
        "  testLoaders.append(testLoader)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9PUVH0PYpWt",
        "outputId": "668446d7-9bc4-40b1-dd1f-615ac70929f6"
      },
      "source": [
        "# Size of train and test datasets\n",
        "for i in range(len(BatchSizes)):\n",
        "  print('No. of samples in train set '+str(i)+\" is \"+str(len(trainLoaders[i].dataset)))\n",
        "  print('No. of samples in test set '+str(i)+\" is \"+str(len(testLoaders[i].dataset)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No. of samples in train set 0 is 60000\n",
            "No. of samples in test set 0 is 10000\n",
            "No. of samples in train set 1 is 60000\n",
            "No. of samples in test set 1 is 10000\n",
            "No. of samples in train set 2 is 60000\n",
            "No. of samples in test set 2 is 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0B15j6-eovUN"
      },
      "source": [
        "## Convolutional Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzXOQ0wUo-7G",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "e77647e0-dd6b-4ccf-ca1c-e00948f6d924"
      },
      "source": [
        "class LeNet(nn.Module):\n",
        "  def __init__(self):    \n",
        "      super(LeNet, self).__init__()\n",
        "      self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n",
        "      self.pool1 = nn.MaxPool2d(kernel_size=2,stride=2)\n",
        "      self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
        "      self.pool2 = nn.MaxPool2d(kernel_size=2,stride=2)        \n",
        "      self.fc1 = nn.Linear(400, 120)\n",
        "      self.fc2 = nn.Linear(120, 84)\n",
        "      self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "      x = F.relu(self.conv1(x))\n",
        "      x = self.pool1(x)\n",
        "      x = F.relu(self.conv2(x))\n",
        "      x = self.pool2(x)\n",
        "      x = x.view(-1, 400)\n",
        "      x = F.relu(self.fc1(x)) \n",
        "      x = F.relu(self.fc2(x))\n",
        "      x = self.fc3(x)\n",
        "      return F.log_softmax(x,dim=1)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n    super(LeNet, self).__init__()\\n    self.conv1 = nn.Conv2d(1,6,5) #1 input image channel, 6 output channels, 5X5 square convolution kernel\\n    self.pool1 = nn.MaxPool2d(2,stride=2)\\n    self.conv2 = nn.Conv2d(6,16,5)\\n    self.pool2 = nn.MaxPool2d(2,stride=2)\\n    self.linear1 = nn.Linear(16*4*4,120) #28-5+1=24, #24/2=12, #12-5+1=8, #8/2=4\\n    self.linear2 = nn.Linear(120,84)\\n    self.linear3 = nn.Linear(84,10)\\n    \\n  def forward(self, x):\\n    x=F.relu(self.conv1(x))\\n    x=self.pool1(x)\\n    x=F.relu(self.conv2(x))\\n    x=self.pool2(x)\\n    x=F.relu(self.linear1(x))\\n    x=F.relu(self.linear2(x))\\n    x=F.relu(self.linear3(x))\\n    x=F.softmax(x,dim=1)\\n    #print(x.size())\\n    return x\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVc2IXxIpL03",
        "outputId": "c0da4a5c-f844-4108-fb5a-02f30f002382"
      },
      "source": [
        "use_gpu = torch.cuda.is_available()\n",
        "net = LeNet()\n",
        "print(net)\n",
        "if use_gpu:\n",
        "  print('GPU is available')\n",
        "  net=net.cuda()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LeNet(\n",
            "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n",
            "GPU is available\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-6u8p4KhBGc"
      },
      "source": [
        "#### Training the data using Stochastic Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFKbIKGGqPpV"
      },
      "source": [
        "#SDG for one batchsize and one learning rate\n",
        "def training_SGD(num_epochs, trainLoader, learning_rate, net, critreion, adam, decay):\n",
        "  train_loss = []\n",
        "  train_acc = []\n",
        "  optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)#SGD\n",
        "  if adam:\n",
        "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)  \n",
        "  for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    running_corr = 0\n",
        "    for i,data in enumerate(trainLoader):\n",
        "      inputs,labels = data\n",
        "      if use_gpu:\n",
        "        inputs, labels = inputs.cuda(), labels.cuda()\n",
        "      #Initializing model gradients to zero\n",
        "      optimizer.zero_grad()\n",
        "      #Data feed-forward through the network\n",
        "      outputs = net(inputs)\n",
        "      #Predicted class is the one with maximum probablity\n",
        "      preds = torch.argmax(outputs, dim=1)\n",
        "      #Finding the loss\n",
        "      loss = critreion(outputs, labels)\n",
        "      #Accumulating the loss for each batch\n",
        "      running_loss += loss\n",
        "      #Accumulate number of correct predictions\n",
        "      running_corr += torch.sum(preds==labels)\n",
        "\n",
        "    totalLoss1 = running_loss/(i+1)\n",
        "    #Calculating gradients\n",
        "    totalLoss1.backward()\n",
        "    #Updating the model parameters\n",
        "    optimizer.step()\n",
        "    if decay:\n",
        "      learning_rate*=0.99\n",
        "      optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
        "\n",
        "    epoch_loss = running_loss.item()/(i+1)#total loss for one epoch\n",
        "    epoch_acc = running_corr.item()/60000\n",
        "\n",
        "    train_loss.append(epoch_loss)#Saving the loss over epochs\n",
        "    train_acc.append(epoch_acc)#saving the accuracy over epochs\n",
        "\n",
        "    print('Epoch{:.0f}/{:.0f}: Training loss: {:.4f}|Training Accuracy: {:.4f}'.format(epoch+1,num_epochs,epoch_loss,epoch_acc*100))\n",
        "  return train_loss,train_acc\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQXj1lZT53t4"
      },
      "source": [
        "#### Plotting loss Vs epoch for various batch sizes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzrvlVem52HW"
      },
      "source": [
        "#plotting loss and accuracy Vs no. of epochs for one learning rate\n",
        "def plot_data(losses,accs,no_of_batch_sizes,num_epochs,learning_rate):\n",
        "  fig = plt.figure(figsize=[15,5]) \n",
        "  \n",
        "  plt.title(\"Train data set with Learning rate: \"+str( learning_rate))   \n",
        "  plt.subplot(121)\n",
        "  for i in range(no_of_batch_sizes):\n",
        "    plt.plot(range(num_epochs),losses[i],'r-',color=colours[i],label='Batch size='+str(BatchSizes[i]))     \n",
        "  plt.legend(loc='upper right')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Training loss/error')\n",
        "  plt.subplot(122)\n",
        "  for i in range(no_of_batch_sizes):\n",
        "    plt.plot(range(num_epochs),accs[i],'g-',color=colours[i],label='Batch size='+str(BatchSizes[i])) \n",
        "  \n",
        "  plt.legend(loc='upper right')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Training Accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_3pWyKC9Uh9"
      },
      "source": [
        "#test data vs batch size\n",
        "def plot_data2(no_batch_sizes, test_losses, test_accs):\n",
        "  fig = plt.figure(figsize=[15,5]) \n",
        "  plt.subplot(121)\n",
        "  plt.plot(no_batch_sizes,test_losses,'r-',label='Loss/error') \n",
        "  plt.legend(loc='upper right')\n",
        "  plt.xlabel('batch size')\n",
        "  plt.ylabel('loss')\n",
        "  plt.subplot(122)\n",
        "  plt.plot(no_batch_sizes,test_accs,'g-',label='Accuracy') \n",
        "  plt.legend(loc='upper right')\n",
        "  plt.xlabel('batch size')\n",
        "  plt.ylabel('accuracy')\n",
        "\n",
        "#test data vs learning rate\n",
        "def plot_data3(no_learning_rates, test_losses, test_accs):\n",
        "  fig = plt.figure(figsize=[15,5]) \n",
        "  plt.subplot(121)\n",
        "  plt.plot(no_learning_rates,test_losses,'r-',label='Loss/error') \n",
        "  plt.legend(loc='upper right')\n",
        "  plt.xlabel('learning rate')\n",
        "  plt.ylabel('loss')\n",
        "  plt.subplot(122)\n",
        "  plt.plot(no_learning_rates,test_accs,'g-',label='Accuracy') \n",
        "  plt.legend(loc='upper right')\n",
        "  plt.xlabel('learning rate')\n",
        "  plt.ylabel('accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fawYtRy9p8M_"
      },
      "source": [
        "def test_SGD(testLoader, net, batch_size, learning_rate):\n",
        "  correct_pred=0\n",
        "  running_loss=0\n",
        "  total_loss=0\n",
        "  \n",
        "  for i,data in enumerate(testLoader):\n",
        "      inputs,labels = data\n",
        "      if use_gpu:\n",
        "          inputs, labels = inputs.cuda(),labels.cuda()\n",
        "      # Feedforward train data batch through model\n",
        "      output = net(inputs)\n",
        "      loss = critreion(output, labels)\n",
        "      total_loss += loss\n",
        "      # Predicted class is the one with maximum probability\n",
        "      preds = torch.argmax(output,dim=1)\n",
        "      correct_pred += torch.sum(preds==labels)\n",
        "      \n",
        "    \n",
        "\n",
        "  test_accuracy = correct_pred.item()/10000.0\n",
        "  print('Testing accuracy (Batch size={:.0f}, lr={:.2f}) = {:.4f}'.format(batch_size, learning_rate, test_accuracy*100)) \n",
        "  return total_loss,test_accuracy\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6e4y4NgP9eR"
      },
      "source": [
        "0.1 is preferable learning rate. If the learning rate is too low then it will take a huge amount of time to converge. If the learning rate is large, it will overshoot the minima and may diverge even.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdKzJH2fkEUN"
      },
      "source": [
        "critreion=nn.CrossEntropyLoss()\n",
        "learning_rates = [0.025,0.05,0.1,0.2,0.5]\n",
        "num_epochs = 10\n",
        "trainLoader=trainLoaders[-1]\n",
        "def SDG_for_different_LR(num_epochs, trainLoaders,testLoaders ,learning_rates, critreion, adam=False, decay=False):\n",
        "  losses=[]\n",
        "  accs=[]\n",
        "  for learning_rate in learning_rates:  \n",
        "    use_gpu = torch.cuda.is_available()\n",
        "    net = LeNet()\n",
        "    print(net)\n",
        "    if use_gpu:\n",
        "      print('GPU is available')\n",
        "      net=net.cuda()\n",
        "    print(\"---------------------------------------------------------------------------------------------------------------------\")\n",
        "    print(\"Learning rate: \", learning_rate)\n",
        "    print(\"---------------------------------------------------------------------------------------------------------------------\")\n",
        "    train_losses=[]\n",
        "    train_accs=[]\n",
        "    test_losses=[]\n",
        "    test_accs=[]\n",
        "    \n",
        "    for i in range(no_of_batch_sizes):\n",
        "      print(\"______________________________________________________________\")\n",
        "      print(\"Batch size: \", BatchSizes[i])\n",
        "      print(\"______________________________________________________________\")\n",
        "      train_loss,train_acc = training_SGD(num_epochs, trainLoaders[i], learning_rate, net, critreion, adam, decay)\n",
        "      test_loss,test_acc=test_SGD(testLoaders[i], net,BatchSizes[i], learning_rate)\n",
        "      train_losses.append(train_loss)\n",
        "      train_accs.append(train_acc)\n",
        "      test_losses.append(test_loss)\n",
        "      test_accs.append(test_acc)      \n",
        "    losses.append(test_losses)\n",
        "    accs.append(test_accs)\n",
        "    plot_data(train_losses,train_accs,no_of_batch_sizes,num_epochs,learning_rate)\n",
        "    plot_data2(BatchSizes, test_losses, test_accs)\n",
        "\n",
        "  \n",
        "  for i in range(no_of_batch_sizes):    \n",
        "    plot_data3(learning_rates,[losses[j][i] for j in range(len(learning_rates))],[accs[j][i] for j in range(len(learning_rates))])\n",
        "    \n",
        "\n",
        "    \n",
        "    \n",
        "  \n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haDFs3u-UzN-"
      },
      "source": [
        "#Schocastic gradient descent with adam\n",
        "SDG_for_different_LR(num_epochs, trainLoaders,testLoaders, learning_rates, critreion)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XeACt1kkGre"
      },
      "source": [
        "#Schocastic gradient descent with decay\n",
        "SDG_for_different_LR(num_epochs, trainLoaders,testLoaders, learning_rates, critreion, decay=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnqMJKP7Ze9R"
      },
      "source": [
        "#Schocastic gradient descent with adam\n",
        "SDG_for_different_LR(num_epochs, trainLoaders,testLoaders, learning_rates, critreion, adam=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpPvx9T0Zsl_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}